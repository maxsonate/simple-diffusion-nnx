# Simple Image Diffusion in NNX
A simple image diffusion model in NNX. NNX is a JAX-based neural network library designed for simplicity and power. Its modular approach follows standard Python conventions, making it both intuitive and compatible with the broader JAX ecosystem. Please note that NNX is experimental and subject to change.

For more information regarding NNX, please refer to https://flax.readthedocs.io/en/v0.8.3/experimental/nnx/index.html

This work was inspired and derived from the implementation of image diffusion model tutorial of Kaggle, originally in Jax. For more information, please refer to the following:
https://www.kaggle.com/code/darshan1504/exploring-diffusion-models-with-jax


# Backbone
A modified version of the U-Net (https://arxiv.org/abs/1505.04597) that uses ResNet blocks instead of simple convolutional blocks. This U-Net will incorporate the time embedding and an additional attention mechanism in every block.


# Training

## Training Loop

From the Kaggle implementation (https://www.kaggle.com/code/darshan1504/exploring-diffusion-models-with-jax?scriptVersionId=98957007&cellId=20):

The standard equation for the backward pass can be given as:

$$p_{\theta}(x_{t-1} | x_{t}) = N(x_{t-1};\mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t}, t)) $$

Here, we aim to generate the noise when going from a state  $$x_{t}$$ to $$x_{t-1}$$ according to the mean $$\mu_{\theta}$$ and standard deviation distribution $$\Sigma_{\theta}$$ generated by the model. Researchers found that fixing the value of the variance to the value of $$\beta_{t}$$ helps the model produce better outputs. Though this is still under active experimentation, we will go ahead and assume the output variance to be set as $$\beta_{t}$$.

Now that we have defined what we need to do, let us define the loss function. The loss function used for diffusion models is derived from the ELBO loss commonly used with variational autoencoders. This loss defines a lower bound objective and a simplified version of the objective can be given as:

$$ Loss_{simplified}(\Theta) = E_{t,x_{0},\epsilon}[||\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha_{t}}}x_{0} + \sqrt{1 - \bar{\alpha_{t}}}\epsilon,t)||^{2}]  \quad \epsilon \in N(0,I) $$

The descent function takes three inputs: the timestamp ($$t$$), the original image ($$x_{0}$$), and some randomly generated Gaussian noise that is to be added to the original image ( $$\epsilon$$). The model then generates the noise in the forward-propagation step that it thinks is added to the image and we calculate the mean squared error between the model output noise and the original noise. This loss value is then used to calculate the gradients and backpropagate through the autoencoder model.
 

Training was done on a single Nvidia T4 GPU. The base model is small, hence larger GPUs are not required.

Use the following command to train the model:

```
python main.py --checkpoint_dir=<ckpt_dir> --batch_size=<bsz>
```

Training is done on MNIST dataset, and by default trains 10 epochs. As a sanity check, at the end of the training, the following loss was achieved:
```
Train loss after epoch 9 :0.04927600920200348
```

# Sampling

From the Kaggle implementation (https://www.kaggle.com/code/darshan1504/exploring-diffusion-models-with-jax?scriptVersionId=98957007&cellId=30):

After completing the training process, we must define an inference loop that can generate new samples for us when provided with Gaussian noise as input. The general algorithm for sampling is given as follows:

1: $$x_{T} \sim N(0,I)$$

2:for t = T, ...,1 do

3: $$z \sim N(0,I) $$ if t > 1, else $$z=0$$

4: $$x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} ( x_{t} - \frac{1-\alpha_{t}}{\sqrt{1 - \bar{\alpha_{t}}}} \epsilon_{\theta}(x_{t}, t)) + \sigma_{t}z$$

5: end for

6: return $$x_{0}$$


Let us run through a loop of sampling. We first sample a random noise that we assume is the  $$x_{T}$$ step image. Then, we simply loop backward from $$T$$ to $$1$$ where we sample the image according to the following formula:


$$x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} ( x_{t} - \frac{1-\alpha_{t}}{\sqrt{1 - \bar{\alpha_{t}}}} \epsilon_{\theta}(x_{t}, t)) + \sigma_{t}z$$

To perform sampling using the above method:
```
python sample.py --checkpoint_dir=<path_to_checkponts> --samples_dir=<path_to_save_imgages>
```

Following are the images generated through the backward denoising (every 25 steps):
![ddpm_sample_0](https://github.com/user-attachments/assets/92ea2e73-30f6-4a52-ad7b-e0a3e8e7e5ea)
![ddpm_sample_25](https://github.com/user-attachments/assets/0203d022-1930-4d5c-9038-ed0c7b2f09a0)
![ddpm_sample_50](https://github.com/user-attachments/assets/9ed49a9b-b1eb-46de-8bf7-2036a2361cfa)
![ddpm_sample_75](https://github.com/user-attachments/assets/97550e2f-022d-446a-919e-355efe0f2f32)
![ddpm_sample_100](https://github.com/user-attachments/assets/ca29aae4-2c72-4277-b026-85edac5ec838)
![ddpm_sample_125](https://github.com/user-attachments/assets/6fde495c-4162-41a0-9a9f-96cfd9395033)
![ddpm_sample_150](https://github.com/user-attachments/assets/4ce37213-5703-447e-be0a-8b04ff1030fa)
![ddpm_sample_175](https://github.com/user-attachments/assets/88f37b8d-ea92-473c-8d06-63a39c4a0fea)
![ddpm_sample_final](https://github.com/user-attachments/assets/6efdf913-57fa-477e-a3ad-3c133ec11848)




# TODOs
- Add DDIM sampling




